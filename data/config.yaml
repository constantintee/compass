databasepsql:
  type: "postgresql"
  host: ""
  port: 
  user: ""
  password: ""
  dbname: ""

downloader:
  download_interval_minutes: 60
  max_retries: 5
  backoff_factor: 2
  retry_initial_delay_seconds: 2
  thread_pool_size: 5
  fetching:
    source_priority:
      - "yfinance"
      - "stooq"
  logging:
    enabled: true
    log_file: "data/logs/data_download.log"
    max_bytes: 5242880  # 5MB
    backup_count: 5
    level: DEBUG

training_logging:
  enabled: true
  file: "data/logs/training.log"
  max_bytes: 5242880  # 5MB
  backup_count: 5
  environment: "development"  # or "production"

main_logging:
  enabled: true
  file: "data/logs/main.log"
  max_bytes: 5242880  # 5MB
  backup_count: 5

webservice_logging:
  enabled: true
  file: "data/logs/webservice.log"
  max_bytes: 5242880  # 5MB
  backup_count: 5

preprocessors:
  - type: 'Preprocessor'
    sequence_length: 60
    cache_dir: 'cache'
    frequency: 'B'
    thread_pool_size: 20

# =============================================================================
# OPTIMIZED MODEL CONFIGURATIONS FOR BEST PREDICTIONS
# =============================================================================

models:
  # Enhanced Attention-LSTM Model
  - type: 'LSTMModel'
    sequence_length: 60
    units: 192              # Larger capacity for complex patterns
    dropout_rate: 0.25      # Balanced regularization
    l2_reg: 0.0005          # Lighter regularization

  # Enhanced Transformer Model with multi-scale attention
  - type: 'TransformerModel'
    sequence_length: 60
    num_heads: 8            # Multi-head attention
    ff_dim: 384             # Increased capacity
    num_transformer_blocks: 6  # Deeper network
    dropout_rate: 0.2       # Moderate dropout

  # CNN-Hybrid Model for local pattern detection
  - type: 'DenseModel'
    sequence_length: 60
    layers:
      - units: 512
        activation: 'relu'
      - units: 256
        activation: 'relu'
      - units: 128
        activation: 'relu'
      - units: 64
        activation: 'relu'

# =============================================================================
# OPTIMIZED TRAINING CONFIGURATION
# =============================================================================

training:
  memory_management:
    chunk_size: 100
    max_memory_percent: 85
    cleanup_threshold: 90

  # Training parameters - Optimized for best predictions
  epochs: 60                    # More epochs for better convergence
  batch_size: 32                # Larger batch for stable gradients
  num_models: 3
  validation_split: 0.2

  # Target metrics
  target_mae: 0.12              # More aggressive target
  max_iterations: 15

  # Learning rate configuration
  initial_learning_rate: 0.0003  # Peak LR after warmup
  min_learning_rate: 0.0000001
  warmup_epochs: 5

  # Early stopping - Increased patience for SWA
  early_stopping:
    patience: 15                # More patience for SWA convergence
    min_delta: 0.0001           # Finer improvement detection
    restore_best_weights: true

  # Learning rate reduction
  reduce_lr:
    factor: 0.5
    patience: 7
    min_lr: 0.0000001

  # Meta-model specific settings
  meta_epochs: 100              # More epochs for meta-model
  meta_learning_rate: 0.001     # Higher LR for meta-model

  # Advanced features
  mixed_precision: true
  use_swa: true                 # Stochastic Weight Averaging
  swa_start_epoch: 20           # Start SWA after epoch 20
  use_cosine_annealing: true    # Cosine annealing with warm restarts

  # Loss function configuration
  loss_function:
    type: 'combined'            # combined, huber, or mse
    huber_weight: 0.4
    mse_weight: 0.4
    direction_weight: 0.2       # Weight for directional accuracy
    huber_delta: 1.0

  # Data augmentation
  augmentation:
    enabled: true
    jitter_sigma: 0.03
    scaling_sigma: 0.1
    augment_probability: 0.3

  years: 7

backtesting_results_path: 'data/backtesting_results'

stocks:
  - "AAPL"
  - "MSFT"
  - "GOOGL"
  - "AMZN"
  - "TSLA"
  - "META"
  - "NVDA"
  - "DIS"
  - "V"
  - "PLTR"
  - "NFLX"
  - "BRK-B"
  - "BABA"
  - "JNJ"
  - "PG"
  - "UNH"
  - "VZ"
  - "JPM"
  - "PFE"
  - "HD"
  - "NKE"
  - "MA"
  - "TSM"
  - "MC.PA"
  - "ASML"
  - "SHEL"
  - "SAP"
  - "BHP"
  - "TM"
  - "MBG.DE"
  - "BMW.DE"
  - "VOW3.DE"
  - "DBK.DE"
  - "BAS.DE"
  - "SIE.DE"
  - "ADS.DE"
  - "IFX.DE"
  - "LIN.DE"
  - "BAYN.DE"
  - "ALV.DE"
  - "RWE.DE"
  - "FME.DE"
  - "FRE.DE"
  - "HEI.DE"
  - "HEN3.DE"
  - "BEI.DE"
  - "MTX.DE"
  - "EON.BD"
  - "ZAL.DE"
  - "LMT"
  - "NOC"
  - "GD"
  - "BA"
  - "RTX"
  - "LHX"
  - "HII"
  - "TXT"
  - "LDOS"
  - "KTOS"
  - "SAAB-B.ST"
  - "AIR.PA"
  - "GDYN"
  - "CSF0.SG"
  - "BA.L"
  - "DRS"
  - "AVAV"
  - "CAE"
  - "RHM.DE"
  - "NEM"
  - "GOLD"
  - "AEM"
  - "WPM"
  - "FNV"
  - "PAAS"
  - "KGC"
  - "SSRM"
  - "HL"
  - "XOM"
  - "CVX"
  - "BP"
  - "TTE"
  - "COP"
  - "EOG"
  - "EQNR"
  - "MRO"
  - "SU"
  - "OXY"
  - "PSX"
  - "VLO"
  - "ENB"
  - "KMI"
  - "CTRA"
  - "FANG"
  - "RIG"
  - "MPC"
  - "BAC"
  - "WFC"
  - "HSBC"
  - "TD"
  - "SAN"
  - "ITUB"
  - "AXP"
  - "MRK"
  - "ABBV"
  - "NVS"
  - "AZN"
  - "SNY"
  - "CSL.AX"
  - "MDT"
  - "KO"
  - "PEP"
  - "UL"
  - "NESN.SW"
  - "DEO"
  - "WMT"
  - "COST"
  - "NEE"
  - "DUK"
  - "NGG"
  - "ENEL.MI"
  - "KEP"
  - "CAT"
  - "DE"
  - "MMM"
  - "ABBN.SW"
  - "GE"
  - "HON"
  - "EMR"
  - "RIO"
  - "VALE"
  - "LYB"
  - "APD"
  - "DOW"
  - "PLD"
  - "AMT"
  - "SPG"
  - "EQIX"
  - "PSA"
  - "SBUX"
  - "MCD"
  - "BKNG"
  - "HMC"
  - "9983.T"
  - "ORLY"
  - "T"
  - "CHL.F"
  - "VOD"
  - "NTES"
  - "ORCL"
  - "INTC"
  - "CSCO"
  - "ERIC"
  - "ACN"
  - "ADBE"
  - "PTRO.JK"
  - "ENI.MI"
  - "CVE"
  - "CJ1.SG"
  - "ONGC.NS"
  - "SO"
  - "D"
  - "EXC"
  - "PCG"
  - "PEG"
  - "RDS.AX"
  - "BIDU"
  - "TCEHY"
  - "SHOP"
  - "INFY"
  - "BAYRY"
  - "MELI"
  - "SPY"
  - "PKX"
